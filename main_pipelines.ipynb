{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # run this once no need to run again\n",
    "# df2 = pd.read_csv('annotation/2.csv', header=None)\n",
    "# df4 = pd.read_csv('annotation/4.csv', header=None)\n",
    "# lab1 = pd.read_csv('annotation/labels_my-project-name_2023-11-23-11-28-38.csv', header=None)\n",
    "# lab2 = pd.read_csv('annotation/labels_my-project-name_2023-11-26-12-25-12.csv', header=None)\n",
    "# # concat all together\n",
    "# # run this once to get proper image.\n",
    "# df = pd.concat([df2, df4, lab1, lab2])\n",
    "# df.columns = ['label', 'x', 'y', 'image_name', 'image_width', 'image_height']\n",
    "# df['image_name'] = df['image_name'].apply(lambda name: 'images/' + name)\n",
    "# df.to_csv('annotation/all.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotations_df = pd.read_csv('annotation/all.csv', index_col=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Code classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ImagePreprocessor(ABC):\n",
    "    @abstractmethod\n",
    "    def preprocess(self, image):\n",
    "        pass\n",
    "\n",
    "    def show_images(self, image):\n",
    "        prepro_img = self.preprocess(image)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20,20))\n",
    "        axes = axes.flatten()\n",
    "        axes[0].imshow(image, cmap='gray', vmin = 0, vmax = 255)\n",
    "        axes[1].imshow(prepro_img, cmap='gray', vmin = 0, vmax = 255)\n",
    "        return prepro_img\n",
    "\n",
    "\n",
    "class BackgroundPreprocessor(ImagePreprocessor):\n",
    "    def __init__(self, background_points, width=1920, height=1080):\n",
    "        self.background_points = background_points\n",
    "        self.background_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        cv2.fillPoly(self.background_mask, [self.background_points], color=255)\n",
    "        self.background_mask = cv2.bitwise_not(self.background_mask)\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        # Create a mask for the background points\n",
    "        return cv2.bitwise_and(image, image, mask=self.background_mask)\n",
    "\n",
    "\n",
    "class ImageAveragingPreprocessor(ImagePreprocessor):\n",
    "    def __init__(self, background_images):\n",
    "        self.background_images = [cv2.GaussianBlur(i, ksize=(7,7), sigmaX=0) for i in background_images]\n",
    "        self.average_image = np.mean(self.background_images, axis=0).astype(np.uint8)\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        return image - self.average_image # image - self.average_image #  cv2.absdiff(image, self.average_image)\n",
    "\n",
    "\n",
    "class ThresholdingPreprocessor(ImagePreprocessor):\n",
    "    def __init__(self, lower_threshold, upper_threshold):\n",
    "        self.lower_threshold = lower_threshold\n",
    "        self.upper_threshold = upper_threshold\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        # Create an inRange mask based on threshold values\n",
    "        inrange_mask = cv2.inRange(image, self.lower_threshold, self.upper_threshold)\n",
    "        img = cv2.bitwise_and(image, image, mask=inrange_mask)\n",
    "\n",
    "        # Apply Otsu's thresholding to further clean the image\n",
    "        _, img = cv2.threshold(img, self.lower_threshold, self.upper_threshold, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        return img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PreprocessingPipeline(ImagePreprocessor):\n",
    "    def __init__(self, processors):\n",
    "        self.processors = processors\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        for processor in self.processors:\n",
    "            image = processor.preprocess(image)\n",
    "        return image\n",
    "\n",
    "class AndBitPreprocessingPipeline(ImagePreprocessor):\n",
    "    def __init__(self, pipeline1, pipeline2):\n",
    "        self.pipeline1 = pipeline1\n",
    "        self.pipeline2 = pipeline2\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        i1, i2 = image.copy(), image.copy()\n",
    "        i1 = self.pipeline1.preprocess(i1)\n",
    "        i2 = self.pipeline2.preprocess(i2)\n",
    "        img = cv2.bitwise_and(i1, i2)\n",
    "        return img\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class GenericDetector(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def detect(self, image):\n",
    "        \"\"\"\n",
    "        Abstract method to detect objects in an image.\n",
    "        This method should be implemented by subclasses to perform actual detection.\n",
    "\n",
    "        Parameters:\n",
    "        image (numpy.ndarray): A grayscale image in which to perform the detection.\n",
    "\n",
    "        Returns:\n",
    "        List[Tuple[int, int]]: A list of (x, y) coordinates for each detected object.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def show_detections(self, image):\n",
    "        \"\"\"\n",
    "        Perform detection on the image and show the image with detected objects marked.\n",
    "\n",
    "        Parameters:\n",
    "        image (numpy.ndarray): The image on which to perform and show detections.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        detections = self.detect(image)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        for x, y in detections:\n",
    "            plt.scatter(x, y, c='green', s=30)\n",
    "        plt.show()\n",
    "\n",
    "class SimpleBlobDetector(GenericDetector):\n",
    "    def __init__(self,\n",
    "                 params = None):\n",
    "        super().__init__()\n",
    "        self.params = cv2.SimpleBlobDetector_Params() if params is None else params\n",
    "        self.detector = cv2.SimpleBlobDetector_create(params)\n",
    "\n",
    "    def detect(self, image):\n",
    "        # Detect blobs.\n",
    "        keypoints = self.detector.detect(image)\n",
    "\n",
    "        # Convert keypoints to (x, y) coordinates\n",
    "        return [(int(k.pt[0]), int(k.pt[1])) for k in keypoints]\n",
    "\n",
    "class HOGSvmDetector:\n",
    "    def __init__(self):\n",
    "        # Initialize HOG descriptor with default people detector\n",
    "        self.hog = cv2.HOGDescriptor()\n",
    "        self.hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "    def detect(self, image):\n",
    "        # Detect people in the image\n",
    "        boxes, _ = self.hog.detectMultiScale(image, winStride=(8, 8), padding=(8, 8), scale=1.05)\n",
    "        center_points = [(int(x + w/2), int(y + h/2)) for (x, y, w, h) in boxes]\n",
    "\n",
    "        return center_points\n",
    "\n",
    "class HaarCascadeDetector(GenericDetector):\n",
    "    def __init__(self, cascade_path):\n",
    "        super().__init__()\n",
    "        self.cascade = cv2.CascadeClassifier(cascade_path)\n",
    "\n",
    "    def detect(self, image):\n",
    "        # Detect objects (faces) in the image\n",
    "        objects = self.cascade.detectMultiScale(image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Extract the top-left corner coordinates of each detected object\n",
    "        return [(x, y) for (x, y, w, h) in objects]\n",
    "\n",
    "class ContourDetector(GenericDetector):\n",
    "    def __init__(self, threshold=200):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def detect(self, image):\n",
    "        # Detect objects (faces) in the image\n",
    "        contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        objects = []\n",
    "        for contour in contours:\n",
    "\n",
    "            # Ignore small contours (adjust the threshold as needed)\n",
    "            if cv2.contourArea(contour) < self.threshold:\n",
    "                continue\n",
    "\n",
    "            # Calculate the center of mass (centroid) of the contour\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                objects.append((cx, cy))\n",
    "\n",
    "        return objects\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Evaluation(ABC):\n",
    "    @abstractmethod\n",
    "    def evaluate(self, image, ground_truth, predictions):\n",
    "        pass\n",
    "\n",
    "    def show_result(self, image, ground_truth, predictions):\n",
    "        results = self.evaluate(image, ground_truth, predictions)\n",
    "        matrix = confusion_matrix([1]*len(ground_truth) + [0]*len(predictions),\n",
    "                                  [1 if result else 0 for result in results])\n",
    "        sns.heatmap(matrix, annot=True, fmt='g')\n",
    "        plt.show()\n",
    "\n",
    "class NearestPointEuclideanEvaluation(Evaluation):\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def evaluate(self, image, ground_truth, predictions):\n",
    "        tp, fp, tn, fn = 0, 0, 0, len(ground_truth)\n",
    "        used_ground_truth = set()  # To keep track of matched ground truth points\n",
    "\n",
    "        for pred in predictions:\n",
    "            # Find the nearest ground truth point that hasn't been used\n",
    "            nearest_dist, nearest_gt = None, None\n",
    "            for gt in ground_truth:\n",
    "                if gt not in used_ground_truth:\n",
    "                    dist = np.linalg.norm(np.array(pred) - np.array(gt))\n",
    "                    if nearest_dist is None or dist < nearest_dist:\n",
    "                        nearest_dist = dist\n",
    "                        nearest_gt = gt\n",
    "\n",
    "            # Check if nearest ground truth is within the threshold\n",
    "            if nearest_dist is not None and nearest_dist <= self.threshold:\n",
    "                tp += 1\n",
    "                fn -= 1\n",
    "                used_ground_truth.add(nearest_gt)  # Mark this ground truth as used\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        return tp, fp, tn, fn\n",
    "\n",
    "class PeopleCount:\n",
    "    def __init__(self, preprocessing_pipeline, detector, evaluator):\n",
    "        self.preprocessing_pipeline = preprocessing_pipeline\n",
    "        self.detector = detector\n",
    "        self.evaluator = evaluator\n",
    "\n",
    "    def run(self, dataframe, show_prediction=False):\n",
    "        detection_results = []\n",
    "        evaluation_results = []\n",
    "\n",
    "        for image_name, group in dataframe.groupby('image_name'):\n",
    "            image = cv2.cvtColor(cv2.imread(image_name), cv2.COLOR_BGR2GRAY)\n",
    "            preprocessed_image = self.preprocessing_pipeline.preprocess(image)\n",
    "\n",
    "            detections = self.detector.detect(preprocessed_image)\n",
    "            ground_truth = list(zip(group['x'], group['y']))\n",
    "            evaluation = self.evaluator.evaluate(preprocessed_image, ground_truth, detections)\n",
    "\n",
    "            detection_results.append({\n",
    "                'image_name': image_name,\n",
    "                'detections': detections\n",
    "            })\n",
    "            evaluation_results.append({\n",
    "                'image_name': image_name,\n",
    "                'evaluation': evaluation\n",
    "            })\n",
    "\n",
    "            if show_prediction:\n",
    "                self.show_predictions(cv2.cvtColor(cv2.imread(image_name), cv2.COLOR_BGR2RGB), ground_truth, detections)\n",
    "\n",
    "        return pd.DataFrame(detection_results), pd.DataFrame(evaluation_results)\n",
    "\n",
    "    def show_predictions(self, image, ground_truth, detections):\n",
    "        for x, y in ground_truth:\n",
    "            cv2.circle(image, (x, y), radius=5, color=(0, 255, 0), thickness=-1)  # Green for ground truth\n",
    "\n",
    "        for x, y in detections:\n",
    "            cv2.circle(image, (x, y), radius=5, color=(0, 0, 255), thickness=-1)  # Red for detections\n",
    "        plt.figure(figsize=(16,12))\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Detections (Red) {len(detections)} vs Ground Truth (Green) {len(ground_truth)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_precision(tp, fp):\n",
    "    \"\"\" Calculate precision based on true positives (TP) and false positives (FP). \"\"\"\n",
    "    if tp + fp == 0:\n",
    "        return 0  # To avoid division by zero\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def calculate_recall(tp, fn):\n",
    "    \"\"\" Calculate recall based on true positives (TP) and false negatives (FN). \"\"\"\n",
    "    if tp + fn == 0:\n",
    "        return 0  # To avoid division by zero\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def display_confusion_matrices(evaluation_results):\n",
    "    total_confusion_matrix = np.zeros((2, 2), dtype=int)  # For the total confusion matrix\n",
    "    all_tp, all_fp, all_tn, all_fn = 0, 0, 0, 0\n",
    "    for index, row in evaluation_results.iterrows():\n",
    "        tp, fp, tn, fn = row['evaluation']\n",
    "        all_tp += tp\n",
    "        all_fp += fp\n",
    "        all_tn += tn\n",
    "        all_tn += fn\n",
    "        precision = calculate_precision(tp=tp, fp=fp)\n",
    "        recall = calculate_recall(tp=tp, fn=fn)\n",
    "        confusion_matrix = np.array([[tp, fp], [fn, tn]])\n",
    "\n",
    "        # Display confusion matrix for each image\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', yticklabels=['Positive', 'Negative'], xticklabels=['Positive', 'Negative'])\n",
    "        plt.title(f\"Confusion Matrix for {row['image_name']}, Precision {precision:.5f}, Recall {recall:.5f}\")\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.show()\n",
    "\n",
    "        # Add to total confusion matrix\n",
    "        total_confusion_matrix += confusion_matrix\n",
    "\n",
    "    # Display total confusion matrix\n",
    "    precision = calculate_precision(tp=all_tp, fp=all_fp)\n",
    "    recall = calculate_recall(tp=all_tp, fn=all_tn)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(total_confusion_matrix, annot=True, fmt='g', cmap='Blues', yticklabels=['Positive', 'Negative'], xticklabels=['Positive', 'Negative'])\n",
    "    plt.title(f\"Total Confusion Matrix, Precision {precision:.5f}, Recall {recall:.5f}\")\n",
    "    plt.xlabel('Ground Truth')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experimentations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bg_points = np.array([\n",
    "    (0, 589),\n",
    "    (27, 529),\n",
    "    (97, 530),\n",
    "    (95, 442),\n",
    "    (137, 446),\n",
    "    (223, 440),\n",
    "    (714, 417),\n",
    "    (854, 402),\n",
    "    (1179, 411),\n",
    "    (1288, 412),\n",
    "    (1384, 418),\n",
    "    (1480, 418),\n",
    "    (1566, 409),\n",
    "    (1920, 398),\n",
    "    (1920, 0),\n",
    "    (0, 0)\n",
    "])\n",
    "\n",
    "bg_pro = BackgroundPreprocessor(bg_points)\n",
    "background1 = cv2.cvtColor(cv2.imread(\"images/10.jpg\"), cv2.COLOR_BGR2GRAY)\n",
    "background2 = cv2.cvtColor(cv2.imread(\"images/9.jpg\"), cv2.COLOR_BGR2GRAY)\n",
    "background1 = bg_pro.preprocess(background1)\n",
    "background2 = bg_pro.preprocess(background2)\n",
    "average_pro = ImageAveragingPreprocessor([background1])\n",
    "threshold_pro = ThresholdingPreprocessor(50, 220)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# example of each pipeline\n",
    "preprocess_pipeline = PreprocessingPipeline([bg_pro, average_pro, threshold_pro])\n",
    "preprocess_pipeline2 = PreprocessingPipeline([bg_pro, ImageAveragingPreprocessor([background2]), threshold_pro])\n",
    "andbitpreprocess_pipeline = AndBitPreprocessingPipeline(preprocess_pipeline, preprocess_pipeline2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# example and how to make blob detector\n",
    "image_exp = cv2.cvtColor(cv2.imread(\"images/1.jpg\"), cv2.COLOR_BGR2GRAY)\n",
    "# image_exp = bg_pro.show_images(image_exp)\n",
    "# image_exp = average_pro.show_images(image_exp)\n",
    "# image_exp = threshold_pro.show_images(image_exp)\n",
    "image_exp = andbitpreprocess_pipeline.show_images(image_exp)\n",
    "\n",
    "params = cv2.SimpleBlobDetector_Params()\n",
    "# Filter by Area.\n",
    "params.filterByArea = True\n",
    "params.minArea = 2\n",
    "params.maxArea = 100\n",
    "\n",
    "# Filter by Circularity\n",
    "params.filterByCircularity = True\n",
    "params.minCircularity = 0.75\n",
    "\n",
    "# Filter by Convexity\n",
    "params.filterByConvexity = True\n",
    "params.minConvexity = 0.75\n",
    "\n",
    "# Filter by Inertia\n",
    "params.filterByInertia = True\n",
    "params.minInertiaRatio = 0.15\n",
    "blob_detector = SimpleBlobDetector(params)\n",
    "blob_detector.show_detections(image_exp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator = NearestPointEuclideanEvaluation(100)\n",
    "pc_solo = PeopleCount(preprocessing_pipeline=preprocess_pipeline,\n",
    "                      detector=blob_detector,\n",
    "                      evaluator=evaluator)\n",
    "detection_df_solo, evaluation_result_solo = pc_solo.run(annotations_df, show_prediction=True)\n",
    "display_confusion_matrices(evaluation_result_solo)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator = NearestPointEuclideanEvaluation(100)\n",
    "pc_dual = PeopleCount(preprocessing_pipeline=andbitpreprocess_pipeline,\n",
    "                      detector=blob_detector,\n",
    "                      evaluator=evaluator)\n",
    "detection_df_dual, evaluation_result_dual = pc_dual.run(annotations_df, show_prediction=True)\n",
    "display_confusion_matrices(evaluation_result_dual)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator = NearestPointEuclideanEvaluation(100)\n",
    "svm_detector = HOGSvmDetector()\n",
    "pc_dualsvm = PeopleCount(preprocessing_pipeline=andbitpreprocess_pipeline,\n",
    "                      detector=svm_detector,\n",
    "                      evaluator=evaluator)\n",
    "detection_df_dualsvm, evaluation_result_dualsvm = pc_dualsvm.run(annotations_df, show_prediction=True)\n",
    "display_confusion_matrices(evaluation_result_dualsvm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator = NearestPointEuclideanEvaluation(100)\n",
    "contour_detector = ContourDetector()\n",
    "pc_dualcontour = PeopleCount(preprocessing_pipeline=andbitpreprocess_pipeline,\n",
    "                         detector=contour_detector,\n",
    "                         evaluator=evaluator)\n",
    "detection_df_dualcontour, evaluation_result_dualcontour = pc_dualcontour.run(annotations_df, show_prediction=True)\n",
    "display_confusion_matrices(evaluation_result_dualcontour)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# initial work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped = annotations_df.groupby(\"image_name\")\n",
    "# Iterate through each group\n",
    "for image_name, group in grouped:\n",
    "    # Read the image\n",
    "    image = Image.open(image_name)  # Load the image using PIL\n",
    "\n",
    "    # Create a matplotlib figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Add points to the image based on \"x\" and \"y\" coordinates\n",
    "    x = group[\"x\"]\n",
    "    y = group[\"y\"]\n",
    "    ax.scatter(x, y, color='red', s=10)  # You can customize the point appearance\n",
    "\n",
    "    # Set axis limits if needed\n",
    "    image_width = group.iloc[0]['image_width']\n",
    "    image_height = group.iloc[0]['image_height']\n",
    "    ax.set_xlim(0, image_width)\n",
    "    ax.set_ylim(0, image_height)\n",
    "\n",
    "    # Add labels or annotations if desired\n",
    "    plt.title(f\"Image: {image_name}\")\n",
    "    plt.xlabel(\"X-coordinate\")\n",
    "    plt.ylabel(\"Y-coordinate\")\n",
    "\n",
    "    # Show the image with points\n",
    "    # Display the image with y-axis inverted\n",
    "    ax.invert_yaxis()  # Invert the y-axis to display the image right-side up\n",
    "    ax.imshow(image)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the two background images (images without people)\n",
    "background1 = cv2.imread(\"images/10.jpg\")\n",
    "background2 = cv2.imread(\"images/9.jpg\")\n",
    "\n",
    "# Initialize the background subtractor with the two backgrounds\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
    "bg_subtractor.apply(background1, learningRate=0)\n",
    "bg_subtractor.apply(background2, learningRate=0)\n",
    "\n",
    "# Group annotations by image_name\n",
    "grouped_annotations = annotations_df.groupby(\"image_name\")\n",
    "# Group annotations by image_name\n",
    "\n",
    "image_names = []\n",
    "x_values = []\n",
    "y_values = []\n",
    "\n",
    "# Iterate through each group (each image)\n",
    "for image_name, group in grouped_annotations:\n",
    "    # Load the current image based on the image_name\n",
    "    current_image = cv2.imread(image_name)\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fg_mask = bg_subtractor.apply(current_image)\n",
    "\n",
    "    # Post-processing to remove noise and enhance the mask\n",
    "    fg_mask = cv2.erode(fg_mask, None, iterations=2)\n",
    "    fg_mask = cv2.dilate(fg_mask, None, iterations=2)\n",
    "\n",
    "    # Find contours in the foreground mask\n",
    "    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Initialize a count for detected people\n",
    "    people_count = 0\n",
    "\n",
    "    for contour in contours:\n",
    "        # Ignore small contours (adjust the threshold as needed)\n",
    "        if cv2.contourArea(contour) < 200:\n",
    "            continue\n",
    "\n",
    "        # Calculate the center of mass (centroid) of the contour\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        else:\n",
    "            cx, cy = 0, 0\n",
    "\n",
    "        # Draw a dot (circle) at the centroid position\n",
    "        cv2.circle(current_image, (cx, cy), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # Increment the people count\n",
    "        people_count += 1\n",
    "\n",
    "        # Append the detection count, x, and y values to the lists\n",
    "        image_names.append(image_name)\n",
    "        x_values.append(cx)\n",
    "        y_values.append(cy)\n",
    "\n",
    "    # Draw annotations (e.g., bounding boxes) on the image\n",
    "    for _, annotation in group.iterrows():\n",
    "        x, y = annotation['x'], annotation['y']\n",
    "        cv2.circle(current_image, (x, y), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the result with the people count and annotations\n",
    "    cv2.putText(current_image, f\"People Count: {people_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Image with People Count and Annotations\", current_image)\n",
    "    # Display the result with the people count and annotations in the Jupyter Notebook\n",
    "    plt.imshow(cv2.cvtColor(current_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Detection Count: {people_count}, Real Count: {group.shape[0]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "result_df = pd.DataFrame({'x': x_values, 'y': y_values, 'image_name': image_names})\n",
    "#\n",
    "# # Release OpenCV windows\n",
    "# cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare annotation with ground truth to see resulting\n",
    "import numpy as np\n",
    "\n",
    "# Load your ground truth and detection results DataFrames\n",
    "annotation_df = pd.read_csv(\"annotation/all.csv\")  # Replace with the actual file paths\n",
    "\n",
    "# Define the distance threshold for classification\n",
    "distance_threshold = 5.0  # Adjust the threshold as needed\n",
    "\n",
    "annotation_df[\"found\"] = False\n",
    "\n",
    "# Iterate through each row of the result_df DataFrame\n",
    "annotation_df[\"found\"] = False\n",
    "\n",
    "# Iterate through each row of the result_df DataFrame\n",
    "for index, result_row in result_df.iterrows():\n",
    "    # Calculate the Euclidean distance for each annotation point using apply\n",
    "    min_dist, idx = annotation_df.apply(\n",
    "        lambda row: (np.sqrt((row[\"x\"] - result_row[\"x\"]) ** 2 + (row[\"y\"] - result_row[\"y\"]) ** 2), row.name),\n",
    "        axis=1).sort_values().iloc[0]\n",
    "\n",
    "    # Check if the minimum distance satisfies the threshold\n",
    "    if min_dist <= distance_threshold:\n",
    "        # Mark the nearest annotation as found using the index\n",
    "        annotation_df.at[idx, \"found\"] = True\n",
    "\n",
    "# Count the number of true positives and false positives\n",
    "true_positives = annotation_df[annotation_df[\"found\"]][\"found\"].sum()\n",
    "false_positives = len(result_df) - true_positives\n",
    "\n",
    "# Count the number of false negatives\n",
    "false_negatives = len(annotation_df) - true_positives\n",
    "\n",
    "# Print the results\n",
    "print(\"True Positives:\", true_positives)\n",
    "print(\"False Positives:\", false_positives)\n",
    "print(\"False Negatives:\", false_negatives)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "11761",
   "language": "python",
   "display_name": "11761"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
